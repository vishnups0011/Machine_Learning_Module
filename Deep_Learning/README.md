# 100DaysOfML - Deep Learning
I am taking up this challenge to learn Deep Learning every day for at-least one hour !!!

Topics will be added as per the Progress.

Reference : PadhAI, now they migrated to GUVI 

## Deep Learning Techniques

### Primitive Neurons

#### MP Neurons

#### Perceptron

- Perceptron Learning [Algorthim](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Perceptron+Learning+Algorithm.pdf).
- Perceptron Learning [How it Works](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Perceptron+Learning+-+Why+it+works_.pdf).
  #### Code

- Check out the code from [here](https://github.com/mankertales/100DaysOfML/blob/master/Deep_Learning/MPNeuronAndPerceptron.ipynb).

#### Sigmoid Neuron

- Sigmoid Neuron Learning [Algorthim](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Sigmoid_Neuron_Learning+The+complete+learning+algorithm.pdf).
- Sigmoid Neuron Learning [How it Works](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Sigmoid_Neuron_Learning+Mathematical+setup+for+the+learning+algorithm.pdf).
- Sigmoid Neuron Learning using [Gradient Descent](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Sigmoid_Neuron_Learning+Deriving+the+Gradient+Descent+Update+Rule.pdf).
- Sigmoid Neuron Learning [Mathematical setup](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Sigmoid_Neuron_Learning+Mathematical+setup+for+the+learning+algorithm.pdf).

  #### Code

- Check out the code from [here](https://github.com/mankertales/100DaysOfML/blob/master/Deep_Learning/mobile91_Sigmoid_Neuron-cross_entropy_loss.ipynb).
- Check out the code from [here](https://github.com/mankertales/100DaysOfML/blob/master/Deep_Learning/mobile91_Sigmoid_Neuron_squared_error_loss.ipynb).

#### Feed Forward Network

- Feed Forward Network [Algorthim](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Sigmoid_Neuron_Learning+The+complete+learning+algorithm.pdf).

  #### Code

- Check out the code from [here for Feed Forward Network](https://github.com/mankertales/100DaysOfML/blob/master/Deep_Learning/FeedForwardNetwork.ipynb).
- Check out the code from [here for Feed Forward Network Generic Class ](https://github.com/mankertales/100DaysOfML/blob/master/Deep_Learning/FeedForwardNetwork_Generic_Class.ipynb).
- Check out the code from [here for Feed Forward Network Multi_class_classification](https://github.com/mankertales/100DaysOfML/blob/master/Deep_Learning/FeedForwardNetwork_Multi_class_classification.ipynb).

#### Back Propagation

- Back Propagation Network [Algorthim](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Sigmoid_Neuron_Learning+The+complete+learning+algorithm.pdf).

  #### Code

- Check out the code from [here for Scalar Backpropagation First Feed Forward Network](https://github.com/mankertales/100DaysOfML/blob/master/Deep_Learning/ScalarBackpropagation_First_FF_Network.ipynb).
- Check out the code from [here for Scalar Backpropagation Multi class classification](https://github.com/mankertales/100DaysOfML/blob/master/Deep_Learning/ScalarBackpropagation_Multi_class_classification.ipynb).

#### Vectorized Feed Forward Networks

- Vectorized Feed Forward Networks [Algorthim](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Sigmoid_Neuron_Learning+The+complete+learning+algorithm.pdf).

#### Code

- Check out the code from [here for Vectorized Feed Forward Networks](https://github.com/mankertales/100DaysOfML/blob/master/Deep_Learning/VectorizedFeedForwardNetworks.ipynb).

### Learning Algorthim

- The idea of stochastic and Mini Batch Gradient Descent [Algorthim](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/The+idea+of+stochastic+and+mini-batch+gradient+descent.pdf).
- Momentum Based Gradient Descent [Algorthim](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Dissecting+the+update+rule+for+momentum+based+gradient+descent.pdf).
- Mini Batch Gradient Descent [Algorthim](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Running+mini-batch+gradient+descent.pdf).
- Stochastic Gradient Descent [Algorthim](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Running+stochastic+gradient+descent.pdf).

### Adaptive Learning Algorthims

- Why do we need [Adaptive Learning Rate](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Why+do+we+need+an+adaptive+learning+rate+_.pdf)
- Adagrad [Learning](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Introducing+Adagrad.pdf)
    - Running Adagrad [Learning](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Running+and+Visualizing+Adagrad.pdf)
- RMSProp [Learning and Visualization](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Running+and+visualizing+RMSProp.pdf)
- Adam [Learning and Visualization](https://github.com/mankertales/100DaysOfML/edit/master/Deep_Learning/Running+and+visualizing+Adam.pdf)
